# LLM Calculator ðŸ§®

Find out how much compute power and storage you need to train your model.

<img src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExaXptNWh3bm54NDZwMDVidXNua2lidGxiZGl5c3VmeWllbmE5NGdueSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/3owzW5c1tPq63MPmWk/200.gif"/>

## References

* [The FLOPS Calculus of Language Model Training](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) by Dzmitry Bahdanau
* [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) by EleutherAI
* [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/) by Kipply
* [New Scaling Laws for Large Language Models](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models) by LessWrong
* [How is LLaMa.cpp possible?](https://finbarr.ca/how-is-llama-cpp-possible/) by Finbarr Timbers
* [AWS ML Infrastructure](https://aws.amazon.com/machine-learning/infrastructure/)